---
layout: post
title: Blog 2
---
## Codex Invite
Got invited to try out Codex today! Looks pretty sweet. Can't wait.
<img src="/assets/images/blog3/codexinvite.png"  width="500">

## Testing GPT-3
Today I was playing around with GPT-3(specifically davinci-instruct). I find that it's plenty capable of understanding the context of a conversation and can detect when the context changes. This can be helpful for identifying parallel conversations. 

It can also detect causal links between seemingly contradictory statements but it took a lot of refinement of asking the right questions in order to get the desired answers. 

The goal is for GPT-3 to see the link between the two statements:
(1) Dogs are loyal.
(2) Dogs are dangerous.

The synthesis of these two statements requires introducing a third idea, which is that dogs want to protect their owners due to their loyalty; which causes them to be dangerous. 

### Fail #1: 
<img src="/assets/images/blog3/firstfail.png"  width="1000">

GPT-3 seems to fail to recognize there's a connection between dogs being loyal and dangerous. But it does rezognize that you can reconcile the two statements.

### Fail #2:
<img src="/assets/images/blog3/fail2.png"  width="1000">

GPT-3 is giving contradicting answers to Fail #1. And then it contradicted itself again at the second question. At the last question, it attributed loyal to "good" and dangerous to "bad" and said that a dog cannot inherently be both "good" and "bad". You can see that I got a little bit pissed in the middle there and started arguing with it a bit. 

### Bingo #1
<img src="/assets/images/blog3/bingo1.png"  width="1000">

I tried again with the questions. It seems that GPT-3 doesn't like absolute statements too much. But it acknowledged that dogs can be causally dangerous due to their loyalty and it synthesized a new idea, which is that dogs can protect thier owners. 

<img src="/assets/images/blog3/makinglink.png"  width="500">

Here's a more concise way of asking that question. Also, I have noticed that a fresh prompt tends to give better answers instead of appending new questions to the prompt and confusing the bot with irrelevant information. 


### Identifying points of disagreement

<img src="/assets/images/blog3/pointsofdisagreement.png"  width="500">

GPT-3 seems to think that axioms are de-facto true statements so no one should disagree on those. It also identifies the disagreement on value to possibly be their outlook on safety.

### Identifying strong arguments

<img src="/assets/images/blog3/facts.png"  width="500">

Not too sure what metrics GPT-3 uses to rank these arguments. Not even too sure about the sources.


### Cross examing arguments

<img src="/assets/images/blog3/formattingneeded.png"  width="250">

I was trying to reproduce the results from before where GPT-3 could establish the causal link between two arguments. After failing a couple of times, I realized GPT-3 was not identifying the correct arguments I was pointing to. So I tried a couple of different ways of formatting. In the future, I will probably need to assign a UUID to every statement. 

<img src="/assets/images/blog3/betterformatting.png"  width="500">

This formatting is better. Tomorrow I will try different formatting of opposing arguments and see what information GPT-3 can extract from them. It is really easy to blame GPT-3 for incompetence when I don't get the desired answers but GPT-3 turns out to be plenty capable. "Programming" with GPT-3 is really all about perfecting the prompts. Also, how can I inform GPT-3 of the formatting scheme in the prompt? I will probably have to read the docs a bit more...